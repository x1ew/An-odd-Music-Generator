{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGE_figcC27s"
      },
      "source": [
        "#***Run***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF-P805LC_lr"
      },
      "outputs": [],
      "source": [
        "!gdown '1TVmI7nhTLjdd_6ghEcjLp_9wW0HWi2oa'\n",
        "!unzip -d data/ /content/FinalProjectMat.zip\n",
        "!gdown '1-0a0eKi5awySLCo9xfqbyMaI8uRCID8q'\n",
        "!gdown '1ndBcjwNS6JwyJe6JDury31ZdDAer4MK6'\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmoQgFAGC_zG"
      },
      "source": [
        "#***Import Data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zohwncu8C3ds"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import os\n",
        "import random\n",
        "import soundfile as sf\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8bleQw-EjoE"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roNRbNF7iy7h"
      },
      "source": [
        "## Create dictionary from files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRE5R27DixBn"
      },
      "outputs": [],
      "source": [
        "notes = dict(enumerate(os.listdir(\"data/piano_triads\")))\n",
        "artifacts = dict(enumerate(os.listdir(\"data/artifacts\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIc5ZXO9lt53"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu9QDQk54Suc"
      },
      "outputs": [],
      "source": [
        "def get_random_note(n):\n",
        "    notes_list = random.sample(list(notes), n)\n",
        "    return notes_list\n",
        "def get_random_noise():\n",
        "    random_noise_file = random.sample(list(artifacts), 1)[0]\n",
        "    return random_noise_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW8zSY_ma2Zk"
      },
      "outputs": [],
      "source": [
        "def append_notes(notes_list, sr):\n",
        "    final_waveform = []\n",
        "    for note in notes_list:\n",
        "        wave, sr = librosa.load('data/piano_triads/' + notes[note], sr=sr)\n",
        "        final_waveform =  np.concatenate((final_waveform, wave))\n",
        "    return final_waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uq0bFO3lWCO"
      },
      "outputs": [],
      "source": [
        "def add_random_noise(random_noise_file, piece, sr):\n",
        "    random_noise, sr = librosa.load('data/artifacts/' + artifacts[random_noise_file], sr=sr)\n",
        "    duration = len(piece)\n",
        "    fixed_len_noise = librosa.util.fix_length(random_noise, duration, mode = 'symmetric')\n",
        "    return fixed_len_noise, fixed_len_noise + piece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HX9eb4slyUu"
      },
      "source": [
        "## Test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctkS19yi4Ymc"
      },
      "outputs": [],
      "source": [
        "sample_rate = 16000\n",
        "notes_list = get_random_note(30)\n",
        "initial_piece = append_notes(notes_list, sample_rate)\n",
        "r_noise = get_random_noise()\n",
        "noise_itself, noisy_piece = add_random_noise(r_noise, initial_piece, sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx-Y6voof9Ye"
      },
      "outputs": [],
      "source": [
        "# Initial piece\n",
        "ipd.Audio(initial_piece, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfCKcJxtFTWZ"
      },
      "outputs": [],
      "source": [
        "# Noise itself\n",
        "ipd.Audio(noise_itself, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c9NbJQ5tbLs"
      },
      "outputs": [],
      "source": [
        "# Noisy piece\n",
        "ipd.Audio(noisy_piece, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk3ZvOHCd9RT"
      },
      "outputs": [],
      "source": [
        "LEN_OF_NOTE = len(append_notes(get_random_note(1), sample_rate))\n",
        "print(LEN_OF_NOTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HGDvuS-ORdN"
      },
      "source": [
        "# Denoising Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "568_89J6AM9N"
      },
      "source": [
        "## Model functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAz_mzMQOBMD"
      },
      "outputs": [],
      "source": [
        "def DAEtrain(model, trainloader, criterion, optimizer, n):\n",
        "    losses = []\n",
        "    model.train()\n",
        "    for item in trainloader:\n",
        "        \n",
        "        X = item['noisy'].to(device)\n",
        "        y = item['normal'].to(device)\n",
        "        output = model(X)\n",
        "        \n",
        "        loss = criterion(output, y)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    return np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bn9Q5uK3eQW"
      },
      "outputs": [],
      "source": [
        "def DAEvalidation(model, testloader, criterion, n):\n",
        "    losses = []\n",
        "    corrects = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in testloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            outputs = model(X)\n",
        "            _, pred = torch.max(outputs, dim=1)\n",
        "            corrects += torch.sum(pred == y)\n",
        "\n",
        "            loss = criterion(outputs, y)\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "    return (corrects.item() / n), np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03chIFqH3jBL"
      },
      "outputs": [],
      "source": [
        "def DAEfit(model, criterion, optimizer, EPOCHS):\n",
        "    losses_train, losses_valid, acc_train, acc_valid, predicts = ([] for i in range(5))\n",
        "    best_acc = 0\n",
        "\n",
        "    for e in range(EPOCHS):\n",
        "        train_l = DAEtrain(model, trainloader, criterion, optimizer, len(trainset))\n",
        "        print(f\"Epoch {e + 1} -- train losses {train_l:.3f}\", end='')\n",
        "        losses_train.append(train_l)  \n",
        "\n",
        "    #     val_c, val_l = validation(model, testloader, criterion, len(testloader.sampler))\n",
        "    #     print(f\" --- valid accuracy {val_c * 100:.3f}   valid losses {val_l:.3f}\")\n",
        "    #     losses_valid.append(val_l)\n",
        "    #     acc_valid.append(val_c)\n",
        "    # return losses_train, losses_valid, acc_train, acc_valid\n",
        "    return losses_train, losses_valid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDKQeXFZtC2g"
      },
      "outputs": [],
      "source": [
        "globprec, globrec, globf1 = 0, 0, 0\n",
        "\n",
        "def show_metrics(model, dataloader, filename=None):\n",
        "    global globprec, globrec, globf1\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            outputs = model(X.to(device))\n",
        "            _, pred = torch.max(outputs, dim=1)\n",
        "            y_pred.append(pred)\n",
        "            y_true.append(y)\n",
        "    y_pred = torch.cat(y_pred).cpu()\n",
        "    y_true = torch.cat(y_true)\n",
        "    creport = classification_report(y_true, y_pred)\n",
        "    print(creport)\n",
        "    globprec, globrec, globf1, _ = precision_recall_fscore_support(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred))\n",
        "    disp.plot()\n",
        "    if filename:\n",
        "        plt.savefig(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58zKX4aizFkQ"
      },
      "outputs": [],
      "source": [
        "def plotplz(filename=None):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
        "    # ax1.title(\"Training and Validation Accuracy\")\n",
        "    ax1.plot(acc_valid,label=\"val\")\n",
        "    ax1.plot(acc_train,label=\"train\")\n",
        "    ax1.set_xlabel(\"iterations\")\n",
        "    ax1.set_ylabel(\"accuracy\")\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(losses_valid,label=\"val\")\n",
        "    ax2.plot(losses_train,label=\"train\")\n",
        "    ax2.set_xlabel(\"iterations\")\n",
        "    ax2.set_ylabel(\"loss\")\n",
        "    ax2.legend()\n",
        "    if filename:\n",
        "        fig.savefig(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ov3yqGNiXW6"
      },
      "source": [
        "## Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opm9_Z4rOVGt"
      },
      "outputs": [],
      "source": [
        "class DAEDataset(Dataset):\n",
        "    def __init__(self, notes_list, sample_rate):\n",
        "        self.notes_list = notes_list\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.notes_list)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        notes_list = self.notes_list[item]['wave']\n",
        "        r_noise = self.notes_list[item]['noise']\n",
        "        piece = append_notes(notes_list, self.sample_rate)\n",
        "        noise_itself, noisy_piece = add_random_noise(r_noise, piece, self.sample_rate)\n",
        "        return {\n",
        "            'noisy': torch.Tensor(noisy_piece),\n",
        "            'normal': torch.Tensor(piece)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgZzSKw21mlA"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "TRAIN_SIZE = 50\n",
        "TEST_SIZE = 10\n",
        "DAE_SAMPLE_RATE = 16000\n",
        "\n",
        "train_dae_notes_list = []\n",
        "for i in range(TRAIN_SIZE):\n",
        "    train_dae_notes_list.append({'wave': get_random_note(30), 'noise': get_random_noise()})\n",
        "    \n",
        "test_dae_notes_list = []\n",
        "for i in range(TEST_SIZE):\n",
        "    test_dae_notes_list.append({'wave': get_random_note(30), 'noise': get_random_noise()})\n",
        "\n",
        "trainset = DAEDataset(train_dae_notes_list, DAE_SAMPLE_RATE)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE)\n",
        "\n",
        "testset = DAEDataset(test_dae_notes_list, DAE_SAMPLE_RATE)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3YunS5cGikf"
      },
      "source": [
        "## Autoencoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PT2W_M2V8Cuu"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(torch.nn.Module) :\n",
        "    def __init__(self, input_dim) :\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(nn.Linear(input_dim, 1024),\n",
        "                                     nn.ReLU(True),\n",
        "                                     nn.Linear(1024, 512),\n",
        "                                     nn.ReLU(True),\n",
        "                                     nn.Linear(512, 256),\n",
        "                                     nn.ReLU(True),\n",
        "                                     nn.Linear(256, 128))\n",
        "    \n",
        "        self.decoder = nn.Sequential(nn.Linear(128, 256),\n",
        "                                     nn.ReLU(True),\n",
        "                                     nn.Linear(256, 512),\n",
        "                                     nn.ReLU(True),\n",
        "                                     nn.Linear(512, 1024),\n",
        "                                     nn.ReLU(True),\n",
        "                                     nn.Linear(1024, input_dim))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        enc = self.encoder(x)\n",
        "        dec = self.decoder(enc)\n",
        "        return dec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8DmGBtsB629"
      },
      "outputs": [],
      "source": [
        "DAEmodel = AutoEncoder(trainset[0]['noisy'].shape[0]).to(device)\n",
        "criterion = nn.MSELoss().to(device)\n",
        "optimizer = torch.optim.Adam(DAEmodel.parameters(), lr=1e-4)\n",
        "# losses_train, losses_valid = DAEfit(DAEmodel, criterion, optimizer, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9TWKPI1fXWK"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqUk6MljbYMu"
      },
      "outputs": [],
      "source": [
        "def noisy_to_normal(piece):\n",
        "    return piece\n",
        "    duration = trainset[0]['noisy'].shape[0]\n",
        "    fixed_len_piece = librosa.util.fix_length(piece, duration, mode = 'constatnt')\n",
        "    denoised = DAEmodel(fixed_len_piece)\n",
        "    return denoised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTnjfbRtfbdC"
      },
      "source": [
        "# Note Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD7KsU-_5OCM"
      },
      "source": [
        "## Model functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T20pVYPg5OCP"
      },
      "outputs": [],
      "source": [
        "def NItrain(model, trainloader, criterion, optimizer, n):\n",
        "    losses = []\n",
        "    corrects = 0\n",
        "    model.train()\n",
        "    for item in trainloader:\n",
        "        \n",
        "        X = item['noisy'].to(device)\n",
        "        y = item['target'].squeeze().to(device)\n",
        "        output = model(X)\n",
        "        _, pred = torch.max(output, dim=1)\n",
        "        corrects += torch.sum(pred == y)\n",
        "        loss = criterion(output, y)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    return (corrects.item() / n)*100, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGLR75Ah5OCQ"
      },
      "outputs": [],
      "source": [
        "def NItest(model, testloader, criterion, n):\n",
        "    losses = []\n",
        "    corrects = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for item in testloader:\n",
        "            \n",
        "            X = item['noisy'].to(device)\n",
        "            y = item['target'].squeeze().to(device)\n",
        "            output = model(X)\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            corrects += torch.sum(pred == y)\n",
        "            loss = criterion(output, y)\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "    return (corrects.item() / n), np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2QsxGhI5OCQ"
      },
      "outputs": [],
      "source": [
        "def NIfit(model, criterion, optimizer, EPOCHS):\n",
        "    losses_train, losses_valid, acc_train, acc_valid, predicts = ([] for i in range(5))\n",
        "    best_acc = 0\n",
        "\n",
        "    for e in range(EPOCHS):\n",
        "        train_c, train_l = NItrain(model, trainloader, criterion, optimizer, len(trainset))\n",
        "        print(f\"Epoch {e + 1} -- train accuracy {train_c:.3f}     train losses {train_l:.3f}\", end='')\n",
        "        losses_train.append(train_l)  \n",
        "        acc_train.append(train_c)\n",
        "\n",
        "        val_c, val_l = NItest(model, testloader, criterion, len(testset))\n",
        "        print(f\" --- valid accuracy {val_c * 100:.3f}   valid losses {val_l:.3f}\")\n",
        "        losses_valid.append(val_l)\n",
        "        acc_valid.append(val_c)\n",
        "    return losses_train, losses_valid, acc_train, acc_valid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQk8QEkTflE_"
      },
      "source": [
        "## Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1RVxIEssV14"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "TRAIN_SIZE = 10 # number of all notes * TRAIN_SIZE\n",
        "TEST_SIZE = 2\n",
        "NI_SAMPLE_RATE = 16000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9pkYpMEsRnG"
      },
      "outputs": [],
      "source": [
        "test_ni_notes_list = []\n",
        "for i in range(TEST_SIZE):\n",
        "    for j in notes:\n",
        "        test_ni_notes_list.append({'note': [j], 'noise': get_random_noise()})\n",
        "\n",
        "train_ni_notes_list = []\n",
        "for i in range(TRAIN_SIZE):\n",
        "    for j in notes:\n",
        "        train_ni_notes_list.append({'note': [j], 'noise': get_random_noise()})\n",
        "\n",
        "train_numpy = []\n",
        "for i in train_ni_notes_list:\n",
        "    piece = append_notes(i['note'], sample_rate)\n",
        "    noise_itself, noisy_piece = add_random_noise(i['noise'], piece, sample_rate)\n",
        "    train_numpy.append({\"note\": noisy_piece, \"target\": i['note']})\n",
        "\n",
        "test_numpy = []\n",
        "for i in test_ni_notes_list:\n",
        "    piece = append_notes(i['note'], sample_rate)\n",
        "    noise_itself, noisy_piece = add_random_noise(i['noise'], piece, sample_rate)\n",
        "    test_numpy.append({\"note\": noisy_piece, \"target\": i['note']})\n",
        "\n",
        "np.save(\"train.npy\", train_numpy)\n",
        "np.save(\"NItest.npy\", test_numpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6QzXNbDjCg0"
      },
      "outputs": [],
      "source": [
        "class NIDataset(Dataset):\n",
        "    def __init__(self, notes_list, sample_rate):\n",
        "        self.notes_list = notes_list\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.notes_list)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        piece = self.notes_list[item]['note']\n",
        "        target = self.notes_list[item]['target']\n",
        "        return {\n",
        "            'noisy': torch.tensor(piece, dtype=torch.float32),\n",
        "            'target': torch.tensor(target, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7N6sg7XfZyP"
      },
      "outputs": [],
      "source": [
        "# train_ni_notes_list = np.load(\"data/NItrain.npy\", allow_pickle=True)\n",
        "# test_ni_notes_list = np.load(\"data/NItest.npy\", allow_pickle=True)\n",
        "train_ni_notes_list = np.load(\"NItrain.npy\", allow_pickle=True)\n",
        "test_ni_notes_list = np.load(\"NItest.npy\", allow_pickle=True)\n",
        "\n",
        "print(\"train samples:\", len(train_ni_notes_list))\n",
        "print(\"test samples:\", len(test_ni_notes_list))\n",
        "\n",
        "trainset = NIDataset(train_ni_notes_list, NI_SAMPLE_RATE)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE)\n",
        "\n",
        "testset = NIDataset(test_ni_notes_list, NI_SAMPLE_RATE)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiwHS324513r"
      },
      "source": [
        "## Note Identifier Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPEmMj0q513s"
      },
      "outputs": [],
      "source": [
        "class NoteIdentifier(torch.nn.Module) :\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(nn.Linear(input_dim, 2048),\n",
        "                                     nn.ReLU(True),\n",
        "                                     nn.Linear(2048, 1024),\n",
        "                                     nn.ReLU(True),\n",
        "                                     nn.Linear(1024, 512),\n",
        "                                     nn.ReLU(True),\n",
        "                                     nn.Linear(512, output_dim))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        enc = self.encoder(x)\n",
        "        return enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATJRQQ8b513t"
      },
      "outputs": [],
      "source": [
        "NImodel = NoteIdentifier(trainset[0]['noisy'].shape[0], len(notes)).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(NImodel.parameters(), lr=1e-4)\n",
        "losses_train, losses_valid, acc_train, acc_valid = NIfit(NImodel, criterion, optimizer, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXqrtpbCBCMu"
      },
      "outputs": [],
      "source": [
        "# torch.save(NImodel.state_dict(), \"checkpoints/NImodel.pth\")\n",
        "torch.save(NImodel.state_dict(), \"NImodel.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oexZ-6N2zFWs"
      },
      "source": [
        "## Utilty functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0_zzA9JzHru"
      },
      "outputs": [],
      "source": [
        "def break_down_piece(piece):\n",
        "    return [piece[i:i + LEN_OF_NOTE] for i in range(0, len(piece), LEN_OF_NOTE)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FWl-eWGzw4R"
      },
      "outputs": [],
      "source": [
        "a = break_down_piece(append_notes(get_random_note(5), sample_rate))\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNt_PE9y0WCQ"
      },
      "outputs": [],
      "source": [
        "def identify_note(note):\n",
        "    inp = torch.tensor(note, dtype=torch.float32).to(device)\n",
        "    output = NImodel(inp)\n",
        "    _, pred = torch.max(output, dim=0)\n",
        "    return pred.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHFnH6ZF0bEg"
      },
      "outputs": [],
      "source": [
        "# NImodel = NoteIdentifier(trainset[0]['noisy'].shape[0], len(notes)).to(device)\n",
        "# NImodel.load_state_dict(torch.load(\"checkpoints/ck.pth\"))\n",
        "NImodel.eval()\n",
        "note = get_random_note(1)\n",
        "noisee = get_random_noise()\n",
        "print(\"input note with noise:\", note)\n",
        "piece = append_notes(note, sample_rate)\n",
        "noise_itself, noisy_piece = add_random_noise(noisee, piece, sample_rate)\n",
        "print(\"prediction of model:\", identify_note(noisy_piece))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySRumFqk3jnX"
      },
      "source": [
        "# Next note prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWQ0qJruSpGr"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tV0R74ElBGA"
      },
      "outputs": [],
      "source": [
        "text = os.listdir(\"data/piano_pieces\")\n",
        "list_notes = ''\n",
        "for i in range(len(text)):\n",
        "    records = open('data/piano_pieces/' + text[i], \"r\")\n",
        "    if i != len(text) - 1:\n",
        "        list_notes += ' <START> / ' + records.readline() + ' / <END> /'\n",
        "    else:\n",
        "        list_notes += ' <START> / ' + records.readline() + ' / <END>'\n",
        "notes = [token.strip() for token in list_notes.split('/')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJYqAzG0SuA6"
      },
      "source": [
        "## create ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgx69lRkLQoG"
      },
      "outputs": [],
      "source": [
        "ngram_n = 5\n",
        "text_sequences = []\n",
        "for i in range(ngram_n,len(notes)+1):\n",
        "  seq = notes[i-ngram_n:i]\n",
        "  text_sequences.append(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFzC7UerSysy"
      },
      "source": [
        "## Tokenizing the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxyJGGl_Mi53"
      },
      "outputs": [],
      "source": [
        "#converting the texts into integer sequence\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "sequences=np.asarray(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChPk2WPQPYch"
      },
      "outputs": [],
      "source": [
        "#vocabulary size\n",
        "vocabulary_size = len(tokenizer.word_counts)+1\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnE38iN1S4mT"
      },
      "source": [
        "## Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_ljsVsMQD2j"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "seq_train, seq_test = train_test_split(sequences, test_size=0.2)\n",
        "\n",
        "X_train = seq_train[:,:-1]\n",
        "y_train = seq_train[:,-1]\n",
        "\n",
        "X_test = seq_test[:,:-1]\n",
        "y_test = seq_test[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aWO_SMOSQxF"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, x, y, vocab_size):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        x = self.x[item]\n",
        "        # y = np.eye(self.vocab_size)[int(self.y[item])]\n",
        "        y = self.y[item]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAoB_YeIa0dh"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "trainset = dataset(X_train, y_train, vocabulary_size)\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE)\n",
        "\n",
        "testset = dataset(X_test, y_test, vocabulary_size)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8oR6QB3S_eT"
      },
      "source": [
        "## Model functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR30Is87c7Sk"
      },
      "outputs": [],
      "source": [
        "def S2Strain(model, trainloader, criterion, optimizer, n):\n",
        "    corrects = 0 \n",
        "    losses = []\n",
        "    model.train()\n",
        "    for X, y in trainloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        output = model(X)\n",
        "        _, predict = torch.max(output, dim = 1)\n",
        "        # corrects += torch.sum(predict == np.argmax(y.cpu()))\n",
        "        corrects += torch.sum(predict == y)\n",
        "\n",
        "\n",
        "        loss = criterion(output, y)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    return (corrects.item() / n), np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8SU38nNc7YO"
      },
      "outputs": [],
      "source": [
        "def S2Stest(model, testloader, criterion, n):\n",
        "    losses = []\n",
        "    corrects = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in testloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "        \n",
        "            outputs = model(X)\n",
        "            _, predict = torch.max(outputs, dim=1)\n",
        "            # corrects += torch.sum(predict == np.argmax(y.cpu()))\n",
        "            corrects += torch.sum(predict == y)\n",
        "    \n",
        "\n",
        "            loss = criterion(outputs, y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return (corrects / n), np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Puhl1HsKc7ap"
      },
      "outputs": [],
      "source": [
        "def S2Sfit(model, criterion, optimizer, EPOCHS):\n",
        "    losses_train, losses_valid, acc_train, acc_valid, predicts = ([] for i in range(5))\n",
        "    best_acc = 0\n",
        "\n",
        "    for e in range(EPOCHS):\n",
        "        train_c, train_l = S2Strain(model, trainloader, criterion, optimizer, len(trainset))\n",
        "        print(f\"Epoch {e + 1} --- train accuracy {train_c * 100:.3f}    train losses {train_l:.3f}\", end='')\n",
        "        losses_train.append(train_l)  \n",
        "        acc_train.append(train_c)  \n",
        "\n",
        "        val_c, val_l = S2Stest(model, testloader, criterion, len(testset))\n",
        "        print(f\" --- valid accuracy {val_c * 100:.3f}   valid losses {val_l:.3f}\")\n",
        "        losses_valid.append(val_l)\n",
        "        acc_valid.append(val_c)\n",
        "    return losses_train, losses_valid, acc_train, acc_valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9KcRhPbTCWt"
      },
      "source": [
        "## Language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqGyk5xtetDs"
      },
      "outputs": [],
      "source": [
        "class NextNote(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=2, bidirectional=False)\n",
        "        self.linear = nn.Linear(hidden_size * (ngram_n-1), vocab_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embedded = self.embed(x)\n",
        "        output, hidden = self.lstm(embedded)\n",
        "        output= output.view(output.size(0), -1)\n",
        "        \n",
        "        output = self.linear(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZScgZEmledM"
      },
      "outputs": [],
      "source": [
        "NNmodel = NextNote(vocab_size=vocabulary_size, embed_size=128, hidden_size=256).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(NNmodel.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "losses_train, losses_valid, acc_train, acc_valid = S2Sfit(NNmodel, criterion, optimizer, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT3q7eKsBCMz"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1hf5cGEBCMz"
      },
      "outputs": [],
      "source": [
        "def get_next_note(notes_list_n):\n",
        "    NNmodel.eval()\n",
        "    while len(notes_list_n) < ngram_n:\n",
        "        notes_list_n.insert(0, 1)\n",
        "    # return torch.tensor(notes_list[-4:])\n",
        "    outputs = NNmodel(torch.tensor(notes_list_n[-(ngram_n-1):]).unsqueeze(0).to(device))\n",
        "    \n",
        "    _, predict = torch.max(outputs, dim=1)\n",
        "    return predict.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st1sIWyh3tQm"
      },
      "source": [
        "# Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StpA1vCi3uz0"
      },
      "outputs": [],
      "source": [
        "sample_rate = 16000\n",
        "notes_list = get_random_note(3)\n",
        "initial_piece = append_notes(notes_list, sample_rate)\n",
        "random_noise = get_random_noise()\n",
        "noise_itself, noisy_piece = add_random_noise(random_noise, initial_piece, sample_rate)\n",
        "final_piece = None\n",
        "piece_limit, counter = 30, 0\n",
        "while True:\n",
        "    denoised_piece = noisy_to_normal(noisy_piece)\n",
        "    sub_notes = break_down_piece(denoised_piece)\n",
        "    print(len(sub_notes))\n",
        "    sub_notes_classes = [identify_note(note) for note in sub_notes]\n",
        "    next_note = get_next_note(sub_notes_classes[:])\n",
        "    notes_list = sub_notes_classes + [next_note]\n",
        "    new_piece = append_notes(notes_list, sample_rate)\n",
        "    if next_note == 1:\n",
        "        final_piece = new_piece\n",
        "        break\n",
        "    else:\n",
        "        noise_itself, noisy_piece = add_random_noise(get_random_noise(), new_piece, sample_rate)\n",
        "        print(noisy_piece.shape)\n",
        "        counter += 1\n",
        "        if counter == piece_limit:\n",
        "            final_piece = new_piece\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emoY9x4QBCM0"
      },
      "outputs": [],
      "source": [
        "sf.write(\"out.wav\", final_piece, sample_rate)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Final_Project_5.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('DataScience')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "6533f1d8fff64ecfa22a7923588dfbb5561fb97a48d8d621197b996446e1aed4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}